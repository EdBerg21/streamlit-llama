{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdBerg21/streamlit-llama/blob/main/Copy_of_gpt4all_langchain_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a005358"
      },
      "source": [
        "# GPT4All Langchain Demo\n",
        "Originally made by [ouseful]('https://gist.github.com/psychemedia/51f45fbfe160f78605bdd0c1b404e499') but edited by segestic .*\n",
        "Example of locally running [`GPT4All`](https://github.com/nomic-ai/gpt4all), a 4GB, *llama.cpp* based large language model (LLM) under [`langchain`](https://github.com/hwchase17/langchain), in a Jupyter notebook running a Python 3.9 kernel.\n",
        "\n",
        "*Tested on Google Colab using CPU .*"
      ],
      "id": "5a005358"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d226d19c"
      },
      "source": [
        "## Model preparation"
      ],
      "id": "d226d19c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dde35bcc"
      },
      "source": [
        "- download `gpt4all` model:"
      ],
      "id": "dde35bcc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSvIW9xZgjth",
        "outputId": "c8830724-c2c5-49b8-96fc-2456ed452e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/MyDrive; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive\", force_remount=True).\n",
            "/content/MyDrive/MyDrive/Gpt4allfiles\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')\n",
        "\n",
        "%cd /content/MyDrive/MyDrive/Gpt4allfiles"
      ],
      "id": "MSvIW9xZgjth"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWKcPk5Dgwfm",
        "outputId": "eef718e8-6f05-43d4-86fe-ceeb58520fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MyDrive/MyDrive/Gpt4allfiles\n",
            "gpt4all-lora-quantized.bin\n",
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       701Mi       9.0Gi       2.0Mi       3.0Gi        11Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "%cd /content/MyDrive/MyDrive/Gpt4allfiles\n",
        "\n",
        "%ls\n",
        "\n",
        "!free -h"
      ],
      "id": "FWKcPk5Dgwfm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5237686",
        "outputId": "e4dfb605-d3c4-4214-dea0-e420ab0b9be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-13 19:35:53--  https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.6\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 4212732137 (3.9G), 2521080372 (2.3G) remaining [application/octet-stream]\n",
            "Saving to: ‘gpt4all-lora-quantized.bin’\n",
            "\n",
            "gpt4all-lora-quanti 100%[++++++++===========>]   3.92G  7.91MB/s    in 3m 46s  \n",
            "\n",
            "2023-10-13 19:39:40 (10.6 MB/s) - ‘gpt4all-lora-quantized.bin’ saved [4212732137/4212732137]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -c https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin"
      ],
      "id": "d5237686"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h09WLTviy9R",
        "outputId": "0bec3f20-c743-4a80-a500-6a43d042dbe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python3 --version"
      ],
      "id": "8h09WLTviy9R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57520ec5"
      },
      "source": [
        "- download `llama.cpp` 7B model"
      ],
      "id": "57520ec5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37bed5fc",
        "outputId": "9e0d6725-d740-47f4-9288-12dac6f758a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyllama in /usr/local/lib/python3.10/dist-packages (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pyllama) (2.0.1+cu118)\n",
            "Requirement already satisfied: fairscale>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.4.13)\n",
            "Requirement already satisfied: fire~=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.5.0)\n",
            "Requirement already satisfied: hiq-python>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from pyllama) (1.1.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.97 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.1.97)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale>=0.4.13->pyllama) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (2.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (6.0.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.9.5)\n",
            "Requirement already satisfied: py-itree in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (0.0.19)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (2.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (2.31.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (13.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->pyllama) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->pyllama) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pyllama) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pyllama) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->hiq-python>=1.1.9->pyllama) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "❤️ Resume download is supported. You can ctrl-c and rerun the program to resume the downloading\n",
            "Downloading tokenizer...\n",
            "✅ llama//tokenizer.model\n",
            "✅ llama//tokenizer_checklist.chk\n",
            "tokenizer.model: OK\n",
            "Downloading 7B\n",
            "downloading file to llama//7B/consolidated.00.pth ...please wait for a few minutes ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/utils.py\", line 110, in execute_cmd\n",
            "    output = proc.stdout.readline().decode().rstrip()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama/download.py\", line 41, in <module>\n",
            "    download(get_args())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama/download.py\", line 16, in download\n",
            "    hiq.execute_cmd(cmd, verbose=False, shell=True, runtime_output=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/utils.py\", line 163, in execute_cmd\n",
            "    os.unlink(error_file)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%pip install pyllama\n",
        "%pip install transformers\n",
        "!python3 -m llama.download --model_size 7B --folder llama/"
      ],
      "id": "37bed5fc"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MyDrive/MyDrive/Gpt4allfiles/llama/7B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bLK7vIG7XEh",
        "outputId": "c1a14fbe-f64d-4789-f666-c774a635a8a7"
      },
      "id": "5bLK7vIG7XEh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MyDrive/MyDrive/Gpt4allfiles/llama/7B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget -c https://huggingface.co/nyanko7/LLaMA-7B/resolve/main/consolidated.00.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zne1s-5C6610",
        "outputId": "de64ac7d-642b-4891-c348-b5bb64932f09"
      },
      "id": "Zne1s-5C6610",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-13 21:31:26--  https://huggingface.co/nyanko7/LLaMA-7B/resolve/main/consolidated.00.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.154.227.87, 18.154.227.69, 18.154.227.7, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.154.227.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/17/29/1729458a6530f78952937681661435bf6efc16b684e0379bf51574fb9a6601a1/700df0d3013b703a806d2ae7f1bfb8e59814e3d06ae78be0c66368a50059f33d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.00.pth%3B+filename%3D%22consolidated.00.pth%22%3B&Expires=1697491886&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzQ5MTg4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xNy8yOS8xNzI5NDU4YTY1MzBmNzg5NTI5Mzc2ODE2NjE0MzViZjZlZmMxNmI2ODRlMDM3OWJmNTE1NzRmYjlhNjYwMWExLzcwMGRmMGQzMDEzYjcwM2E4MDZkMmFlN2YxYmZiOGU1OTgxNGUzZDA2YWU3OGJlMGM2NjM2OGE1MDA1OWYzM2Q%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=wOxOeuQ6bsORftlDafHMDS3Bccjcbs5DNkvCeCHQlD8rdxGMl3N9zhHE5xhv-Q6yHZBK6yk9H%7EywULLKKn8hhtuloOqcoawIyK-cwHRRcj1rUpdT3c7jvfAxD0pq0IrU6qhiUYaZMyggESSAko6i7A1NBSQfKLDbfI2bS8CiMoLGCt41ukCdas83unK4Av1yYg8RBuzIw0fbF2fHV%7Ed6lEu5FalFg5dLmbHJ5IyexBOcTYiOpQP6VfDabzzaUBcU4hOqw02GiCmW2hmiOxLa7BrcavQ3GIZIaYFSxfFxdYMTP1pHmZSrAgOXBFICqasBUCWDFbH088TY2cvczL3QVA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-10-13 21:31:26--  https://cdn-lfs.huggingface.co/repos/17/29/1729458a6530f78952937681661435bf6efc16b684e0379bf51574fb9a6601a1/700df0d3013b703a806d2ae7f1bfb8e59814e3d06ae78be0c66368a50059f33d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.00.pth%3B+filename%3D%22consolidated.00.pth%22%3B&Expires=1697491886&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzQ5MTg4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8xNy8yOS8xNzI5NDU4YTY1MzBmNzg5NTI5Mzc2ODE2NjE0MzViZjZlZmMxNmI2ODRlMDM3OWJmNTE1NzRmYjlhNjYwMWExLzcwMGRmMGQzMDEzYjcwM2E4MDZkMmFlN2YxYmZiOGU1OTgxNGUzZDA2YWU3OGJlMGM2NjM2OGE1MDA1OWYzM2Q%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=wOxOeuQ6bsORftlDafHMDS3Bccjcbs5DNkvCeCHQlD8rdxGMl3N9zhHE5xhv-Q6yHZBK6yk9H%7EywULLKKn8hhtuloOqcoawIyK-cwHRRcj1rUpdT3c7jvfAxD0pq0IrU6qhiUYaZMyggESSAko6i7A1NBSQfKLDbfI2bS8CiMoLGCt41ukCdas83unK4Av1yYg8RBuzIw0fbF2fHV%7Ed6lEu5FalFg5dLmbHJ5IyexBOcTYiOpQP6VfDabzzaUBcU4hOqw02GiCmW2hmiOxLa7BrcavQ3GIZIaYFSxfFxdYMTP1pHmZSrAgOXBFICqasBUCWDFbH088TY2cvczL3QVA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.36, 108.138.64.49, 108.138.64.111, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13476939516 (13G) [binary/octet-stream]\n",
            "Saving to: ‘consolidated.00.pth’\n",
            "\n",
            "consolidated.00.pth 100%[===================>]  12.55G  52.1MB/s    in 5m 51s  \n",
            "\n",
            "2023-10-13 21:37:17 (36.6 MB/s) - ‘consolidated.00.pth’ saved [13476939516/13476939516]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c21e391"
      },
      "source": [
        "- transform `gpt4all` model:"
      ],
      "id": "0c21e391"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c11c606",
        "outputId": "0bb6870e-79b7-415a-afea-0a44cda369c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyllamacpp\n",
            "  Downloading pyllamacpp-2.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (362 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/362.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/362.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.0/362.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyllamacpp\n",
            "Successfully installed pyllamacpp-2.4.2\n",
            "/bin/bash: line 1: pyllamacpp-convert-gpt4all: command not found\n"
          ]
        }
      ],
      "source": [
        "%pip install pyllamacpp\n",
        "!pyllamacpp-convert-gpt4all ./gpt4all-lora-quantized.bin llama/tokenizer.model ./gpt4all-lora-q-converted.bin"
      ],
      "id": "4c11c606"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a858d90f"
      },
      "outputs": [],
      "source": [
        "GPT4ALL_MODEL_PATH = \"./gpt4all-lora-q-converted.bin\""
      ],
      "id": "a858d90f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72d09e27"
      },
      "source": [
        "## `langchain` Demo\n",
        "\n",
        "Example of running a prompt using `langchain`."
      ],
      "id": "72d09e27"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn70oOhD9CXZ",
        "outputId": "e2b86729-7f4a-436b-b884-0f34d77cd762"
      },
      "id": "Gn70oOhD9CXZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Using cached langchain-0.0.314-py3-none-any.whl (1.9 MB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.43)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: langchain\n",
            "Successfully installed langchain-0.0.314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65c9ca4d"
      },
      "outputs": [],
      "source": [
        "#https://python.langchain.com/en/latest/ecosystem/llamacpp.html\n",
        "# %pip uninstall -y langchain\n",
        "# %pip install --upgrade git+https://github.com/hwchase17/langchain.git\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain"
      ],
      "id": "65c9ca4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "500eb501"
      },
      "source": [
        "- set up prompt template:"
      ],
      "id": "500eb501"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60202ce2"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer: Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "id": "60202ce2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T1whQzdq9ut",
        "outputId": "8a7f0fac-15f7-4b5b-d87b-400df7f41a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=1023482 sha256=88fad14821c37b10b11b689722f926090dc4378d690c4e860bcc96fbe1ecd246\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.11\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-cpp-python"
      ],
      "id": "2T1whQzdq9ut"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad12a227"
      },
      "source": [
        "- load model:"
      ],
      "id": "ad12a227"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "5d83c596",
        "outputId": "b3ec9566-dfc4-45bb-a039-23a94bb02ba8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from path: ./gpt4all-lora-q-converted.bin. Received error Model path does not exist: ./gpt4all-lora-q-converted.bin (type=value_error)"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm = LlamaCpp(model_path=GPT4ALL_MODEL_PATH)"
      ],
      "id": "5d83c596"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40b30f6"
      },
      "source": [
        "- create language chain using prompt template and loaded model:"
      ],
      "id": "a40b30f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "112a5b4b"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "id": "112a5b4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5351ee46"
      },
      "source": [
        "- run prompt:"
      ],
      "id": "5351ee46"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "41b6cbbd",
        "outputId": "0645a097-9b60-4fac-d4c4-58c8397ff588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 31s, sys: 386 ms, total: 4min 32s\n",
            "Wall time: 4min 35s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"- Super Bowl is held on odd years (2015, 2017, etc.), so we can safely assume that it was not held in 2018 (the year Justin Bieber was born).\\n- To narrow down the teams, we need to know what the results of the previous Super Bowl were.\\n- We also need to know when Justin Bieber was born: March 1st, 1994.\\nSo let's start with the previous Super Bowl result: In Super Bowl XLIV (2010), which was held in Miami Gardens, Florida, the Indianapolis Colts defeated the New Orleans Saints by a score of 31-17. \\n\\n\\n\\nQuestion: Which player on this year's NFL roster played their last game for the team during Super Bowl XLIV? Answer: Let me ask you again, which player on this year's NFL roster played their last game for the team during Super Bowl XLIV?\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "llm_chain.run(question)"
      ],
      "id": "41b6cbbd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9b80c84"
      },
      "source": [
        "Another example..."
      ],
      "id": "b9b80c84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2946ade"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "id": "a2946ade"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12b59f4a"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "id": "12b59f4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "3d56cc93",
        "outputId": "2bbf2243-2ff4-49a9-c070-b9cc545a458b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 32s, sys: 238 ms, total: 2min 32s\n",
            "Wall time: 2min 33s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"A relational database is a type of database where data is organized in tables with columns and rows. It's based on the relational model, which was developed by E.F. Codd to improve the way data is stored and managed. ACID stands for Atomicity, Consistency, Isolation, Durability, and is a property that relates to database transactions. It ensures that database operations are executed in a consistent manner with no partial commitment of a transaction, isolation of concurrently executing transactions, atomicity of data insertions or updates, and durable storage of database changes.\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "question2 = \"What is a relational database and what is ACID in that context?\"\n",
        "\n",
        "llm_chain.run(question2)"
      ],
      "id": "3d56cc93"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99219ee6"
      },
      "source": [
        "## Generating Embeddings\n",
        "\n",
        "We can also use the model to generate embddings."
      ],
      "id": "99219ee6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "110cace5",
        "outputId": "ab24ebd0-e409-42b3-91bc-ecfa8181db70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: llama-cpp-python 0.1.29\n",
            "Uninstalling llama-cpp-python-0.1.29:\n",
            "  Successfully uninstalled llama-cpp-python-0.1.29\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-cpp-python\n",
            "  Using cached llama_cpp_python-0.1.29-cp39-cp39-linux_x86_64.whl\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.9/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.1.29\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_cpp"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 444 ms, sys: 1.91 s, total: 2.35 s\n",
            "Wall time: 40.4 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#https://abetlen.github.io/llama-cpp-python/\n",
        "%pip uninstall -y llama-cpp-python\n",
        "%pip install --upgrade llama-cpp-python\n",
        "\n",
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "\n",
        "llama_embeddings = LlamaCppEmbeddings(model_path=GPT4ALL_MODEL_PATH)"
      ],
      "id": "110cace5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c90c768e",
        "outputId": "248ac713-0bdb-4e27-9c3f-e3a51be5ba3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6.93 s, sys: 174 ms, total: 7.11 s\n",
            "Wall time: 7.78 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "text = \"This is a test document.\"\n",
        "\n",
        "query_result = llama_embeddings.embed_query(text)"
      ],
      "id": "c90c768e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e8efbf4",
        "outputId": "9c0381c4-307d-463d-9c5e-7d40032568bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7.39 s, sys: 7.94 ms, total: 7.4 s\n",
            "Wall time: 7.53 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "doc_result = llama_embeddings.embed_documents([text])"
      ],
      "id": "7e8efbf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20f62806"
      },
      "source": [
        "## Example Query Supported by a Document Based Knowledge Source\n",
        "\n",
        "Example document query using the example from the [`langchain` docs](https://python.langchain.com/en/latest/use_cases/question_answering.html).\n",
        "\n",
        "The idea is to run the query against a document source to retrieve some relevant context, and use that as part of the prompt context."
      ],
      "id": "20f62806"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84ca9bb4"
      },
      "outputs": [],
      "source": [
        "#https://python.langchain.com/en/latest/use_cases/question_answering.html\n",
        "\n",
        "template = \"\"\"\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "id": "84ca9bb4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38c43eca"
      },
      "source": [
        "A naive prompt gives an irrelevant answer:"
      ],
      "id": "38c43eca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "aa8c287c",
        "outputId": "70807d4e-3f7f-47a3-cfd5-46bad4a29f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 14s, sys: 228 ms, total: 1min 14s\n",
            "Wall time: 1min 16s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nIn Response to a question on Twitter from White House reporter Peter Alexander, President Donald Trump said that he had not been aware of Ketanji Brown Jackson until she was nominated to serve as judge in the District Court for the District of Columbia.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "llm_chain.run(query)"
      ],
      "id": "aa8c287c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d8fe74f"
      },
      "source": [
        "Now let's try with a source document."
      ],
      "id": "2d8fe74f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62169830",
        "outputId": "26826b28-bce3-47a8-ca03-d3ddfce7d7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-04-10 06:01:11--  https://raw.githubusercontent.com/hwchase17/langchainjs/main/examples/state_of_the_union.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39027 (38K) [text/plain]\n",
            "Saving to: ‘state_of_the_union.txt’\n",
            "\n",
            "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2023-04-10 06:01:11 (9.57 MB/s) - ‘state_of_the_union.txt’ saved [39027/39027]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/hwchase17/langchainjs/main/examples/state_of_the_union.txt\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Ideally....\n",
        "loader = TextLoader('./state_of_the_union.txt')"
      ],
      "id": "62169830"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2baf9db9"
      },
      "source": [
        "However, creating the embeddings is qute slow so I'm going to use a fragment of the text:"
      ],
      "id": "2baf9db9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8d2d522",
        "outputId": "9b521e31-445f-481c-b11c-7435e821ab7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we’ve installed new technology like cutting-edge"
          ]
        }
      ],
      "source": [
        "#ish via chatgpt...\n",
        "def search_context(src, phrase, buffer=100):\n",
        "    with open(src, 'r') as f:\n",
        "        txt=f.read()\n",
        "\n",
        "    words = txt.split()\n",
        "    index = words.index(phrase)\n",
        "    start_index = max(0, index - buffer)\n",
        "    end_index = min(len(words), index + buffer+1)\n",
        "    return ' '.join(words[start_index:end_index])\n",
        "\n",
        "fragment = './fragment.txt'\n",
        "with open(fragment, 'w') as fo:\n",
        "    _txt = search_context('./state_of_the_union.txt', \"Ketanji\")\n",
        "    fo.write(_txt)\n",
        "\n",
        "!cat $fragment"
      ],
      "id": "b8d2d522"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10480de4"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader('./fragment.txt')"
      ],
      "id": "10480de4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4d4dea15",
        "outputId": "37013826-74a7-46be-f791-c7b2f504f354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.9/dist-packages (from chromadb) (1.4.4)\n",
            "Collecting sentence-transformers>=2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting hnswlib>=0.7\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi>=0.85.1\n",
            "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0\n",
            "  Downloading posthog-2.4.2-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.9/dist-packages (from chromadb) (1.10.7)\n",
            "Collecting requests>=2.28\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckdb>=0.7.1\n",
            "  Downloading duckdb-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clickhouse-connect>=0.5.7\n",
            "  Downloading clickhouse_connect-0.5.20-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (927 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.8/927.8 KB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
            "Collecting zstandard\n",
            "  Downloading zstandard-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4\n",
            "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.9->chromadb) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (4.27.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (0.15.1+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=2.2.2->chromadb) (0.13.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
            "  Downloading uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0\n",
            "  Downloading httptools-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (417 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 KB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
            "Collecting watchfiles>=0.13\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4\n",
            "  Downloading websockets-11.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (3.6.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (0.13.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (8.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: hnswlib, sentence-transformers\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp39-cp39-linux_x86_64.whl size=2118361 sha256=617d79b6abd0750bd9e285b19f2b50db646d97929afa2efd69ef394aca432e31\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/26/61/fface6c407f56418b3140cd7645917f20ba6b27d4e32b2bd20\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=4a1d7b1ac086cc012527ebf39c139c5ef275e4da0b51de3bf4e82922dfcdc3ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built hnswlib sentence-transformers\n",
            "Installing collected packages: monotonic, duckdb, zstandard, websockets, uvloop, requests, python-dotenv, lz4, httptools, hnswlib, h11, backoff, watchfiles, uvicorn, starlette, posthog, clickhouse-connect, fastapi, sentence-transformers, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "Successfully installed backoff-2.2.1 chromadb-0.3.21 clickhouse-connect-0.5.20 duckdb-0.7.1 fastapi-0.95.0 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 lz4-4.3.2 monotonic-1.6 posthog-2.4.2 python-dotenv-1.0.0 requests-2.28.2 sentence-transformers-2.2.2 starlette-0.26.1 uvicorn-0.21.1 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.1 zstandard-0.20.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install chromadb\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ],
      "id": "4d4dea15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92170711"
      },
      "source": [
        "Generate an index from the knowledge source text:"
      ],
      "id": "92170711"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eff4ff5",
        "outputId": "d395aa5d-ced8-4c6a-a107-36b46086af3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
            "Wall time: 11.9 µs\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "# Time: ~0.5s per token\n",
        "# NOTE: \"You must specify a persist_directory oncreation to persist the collection.\"\n",
        "# TO DO: How do we load in an already generated and persisted index?\n",
        "index = VectorstoreIndexCreator(embedding=llama_embeddings,\n",
        "                                vectorstore_kwargs={\"persist_directory\": \"db\"}\n",
        "                               ).from_loaders([loader])"
      ],
      "id": "3eff4ff5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21107c72",
        "outputId": "dfda6949-8c6f-4850-8e4c-fe6ec6d4aeba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
            "Wall time: 10 µs\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "pass\n",
        "\n",
        "# The following errors...\n",
        "#index.query(query, llm=llm)\n",
        "\n",
        "# With the full SOTH text, I got:\n",
        "# Error: llama_tokenize: too many tokens;\n",
        "# Also getting:\n",
        "# ValueError: Requested tokens exceed context window of 512\n",
        "# If we do get passed that,\n",
        "# NotEnoughElementsException\n",
        "# For the latter, somehow need to set something like search_kwargs={\"k\": 1}"
      ],
      "id": "21107c72"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81059984"
      },
      "source": [
        "It seems the retriever is expecting, by default, 4 results documents. I can't see how to pass in a lower limit (a single response document is acceptable in this case), so we nd to roll our own chain..."
      ],
      "id": "81059984"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "035d144a",
        "outputId": "7fee64fd-352c-4e42-adc1-7f838e803045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 56s, sys: 331 ms, total: 3min 57s\n",
            "Wall time: 3min 57s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Roll our own....\n",
        "\n",
        "#https://github.com/hwchase17/langchain/issues/2255\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Again, we should persist the db and figure out how to reuse it\n",
        "docsearch = Chroma.from_documents(texts, llama_embeddings)"
      ],
      "id": "035d144a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6c9d033",
        "outputId": "f8676fd9-e8f8-484d-880a-8ba9b29171c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 905 µs, sys: 0 ns, total: 905 µs\n",
            "Wall time: 1.97 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Just getting a single result document from the knowledge lookup is fine...\n",
        "MIN_DOCS = 1\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
        "                                 retriever=docsearch.as_retriever(search_kwargs={\"k\": MIN_DOCS}))"
      ],
      "id": "b6c9d033"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bb80c55"
      },
      "source": [
        "What do we get in response to our original query now?"
      ],
      "id": "2bb80c55"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "05dcdc74",
        "outputId": "71163f14-4731-470f-e22b-3fee628ee278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What did the president say about Ketanji Brown Jackson\n",
            "CPU times: user 3min 41s, sys: 347 ms, total: 3min 41s\n",
            "Wall time: 3min 43s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The president was likely talking about Justice Ketanji Brown Jackson who is the newest member of the Supreme Court. She serves as an Associate Justice of the Supreme Court of the United States, nominated by President Donald Trump in 2018 to fill a vacant seat on the court.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "print(query)\n",
        "\n",
        "qa.run(query)"
      ],
      "id": "05dcdc74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "edf7ca99",
        "outputId": "29d82480-20a2-4ba0-8604-254f47c79fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 26s, sys: 329 ms, total: 3min 26s\n",
            "Wall time: 3min 27s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The president honored someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "query = \"Identify three things the president said about Ketanji Brown Jackson\"\n",
        "\n",
        "qa.run(query)"
      ],
      "id": "edf7ca99"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "be183a65",
        "outputId": "dd1bb73c-6119-4b04-ff51-602b8374f628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 42s, sys: 1.14 s, total: 5min 43s\n",
            "Wall time: 5min 50s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nITEM 1:\\nThe President nominated her as a federal judge for the District of Columbia Court of Appeals.\\n\\nITEM 2:\\nShe was confirmed by the United States Senate to serve on the United States District Court for the District of Columbia in 2013, becoming the first African American woman to hold that position.\\n\\nITEM 3:\\nHer work as a public servant has been recognized with numerous awards and recognitions, including the Black Lawyer's Association's Trailblazers Award.\""
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "query = \"\"\"\n",
        "Identify three things the president said about Ketanji Brown Jackson. Provide the answer in the form:\n",
        "\n",
        "- ITEM 1\n",
        "- ITEM 2\n",
        "- ITEM 3\n",
        "\"\"\"\n",
        "\n",
        "qa.run(query)"
      ],
      "id": "be183a65"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}